1. 为什么围棋队人工智能很重要？

人工智能的终极目标就是用机器实现人类（部分）智能。
人工智能正式诞生的元年，即1956年，Arthur Samuel (1901-1999, 阿瑟.萨缪尔）就编写了一个程序下西洋跳棋战胜了他自己；
1962年，这个程序击败了一个美国的西洋跳棋州冠军；
1994年，一个新的西洋跳棋程序Chinook战胜了人类总冠军。
1997年，IBM开发的Deep Blue击败了国际象棋世界冠军卡斯帕罗夫。

围棋是智力游戏中搜索空间最大的，所需要的计算量也是最大的。
围棋一共有19*19=361个各自，每个各自3个状态，一共有3^361，将近5千万个状态；
下棋步骤来看，第n步有361-n中选择，至少有361！超过10^200中可能性。

2. AlphaGo Zero的前世

通西洋跳棋和国象一样。围棋可以被建模成一个搜索问题。
下棋的步骤可以表示成一个搜索树，其中每个节点代表一个棋盘状态。
根节点是空白棋盘，每个节点的子节点是在当前棋盘下采取一个行动，即再多下一个棋子，而每一个叶子节点是一个下满了棋子的棋盘状态。
机器要做的事情，就是从当前节点中，尽量找到一条路径，到达能够使己方赢的一个叶子节点。

为了找到路径，人工智能传统的解决方案是“搜索”。所使用的技术包括经典的深度优先搜索，启发式搜索，剪枝等。这个方案能解决很多简单智力游戏，但由于计算量太大，并不能解决复杂问题比如国象和围棋。所以，只能退而求其次，在当前节点（即棋盘状态）中，找到一个最优或较优的子节点。因此，研究者们提出一个新的方案 - "评价"。在这个方案里，不再苛求搜索到一条最佳路径到达赢的叶子节点，而是对每一个节点，估算出一个最佳的子节点。也就是说，对每一个当前的棋盘状态，评价出最佳的下一步行动。

评价的对象主要有两种：一是棋盘状态的“策略”，即遇到当前状态，选择每一个子节点的概率分布如何。其输入是一个棋盘状态外加一个可选的行动，而其输出是在当前状态下选择该行动的概率；二是棋盘状态的“赢面估算”，即每个子节点赢的概率有多大。其输入是一个棋盘状态，而其输出是这个状态能赢的可能性。在理想的情况下，这两者应该是一致的，即选择概率高的子节点赢面就大，反之亦然。然而，实际上这两者往往有些差别。

评价的方法就五花八门了，启发式函数、贝叶斯、蒙特卡洛方法、深度学习等等，不一而足。在AlphaGo Zero中，蒙特卡洛方法和深度学习至少同样重要。蒙特卡洛树搜索在每个节点上做很多次随机游戏。每次随机游戏随机选取一条到达叶子结点的路径，也就是每一步都随机落一个子。最后，该节点的赢面就可以被估算为这么多词随机游戏中赢的百分比。虽然简单，但蒙特卡洛树搜索方法却相当有效。

基于评价的方案对很多智力游戏都行之有效，包括西洋跳棋，国象，五子棋等。然而，由于19*19围棋搜索空间太大。所以，受限于计算能力，在AlphaGo Zero出来之前，基于评价的方案还无法解决这个问题。

3. AlphaGo Zero的今生

然而，AlphaGo Zero横空出世，颠覆了人们的认知。
2015年10月，AlphaGo 5:0击败了欧洲围棋冠军樊麾；
2016年3月，AlphaGo 4:1击败了前世界冠军李世石；
2017年5月，AlphaGo 3:0击败了世界排名第一的中国选手柯洁。
AlphaGo的改版程序Master在各大围棋平台上60:0不败战绩横扫各路围棋豪杰。
2017年10月，AlphaGo自我创新，提出了一个新的程序AlphaGo Zero。该程序完全不依赖人类专家的对局，从零开始，使用强化学习的方法，仅用一台带有4个TPU的机器，40天下了两千九百万局棋，从中进化成一个新的围棋界的“独孤求败”， 100:0吊打李世石版本AlphaGo，89:11痛殴Master。

从技术角度看，AlphaGo Zero继承并改进了之前的框架。AlphaGo Zero依然把围棋建模成一个搜索问题，依然采用基于评价的方案来做决策。其主要学术贡献在于提出了一个新的估算评价函数的方法，更好地融合并改善了已有的两种重要方法（即蒙特卡洛树搜索方法和基于深度学习的方法）。

AlphaGo的评价方法分成四步：
第一，AlphaGo以大量人类专家对局为数据，采用有监督学习的方法学习了一个深度神经网络SL来估算策略；
第二，在SL的基础上，AlphaGo和自己左右互博，采用强化学习的方法把这个策略神经网络改进成一个新的深度神经网络RL；
第三，AlphaGo再在RL的基础上，还是和自己左右互博，还是采用强化学习的方法学习了一个深度神经网络V来估算每个棋盘状态的赢面；
第四，再使用基于蒙特卡洛树搜索方法的算法，巧妙加权融合了策略网络SL/RL和赢面估算网络V, 来做最终的决策。

和AlphaGo相比，AlphaGo Zero作出了很多重要的调整。简而言之，去掉了第一步，而把剩下的三步合并成一步“基于蒙特卡洛树搜索的强化学习”。AlphaGo Zero把中间的策略网络SL/RL和赢面估计网络V合并成了一个深度神经网络，减少了评价方法的复杂度。同时，AlphaGo Zero改进了蒙特卡洛树搜索方法，在每一步随机选取行动的时候，不再是完全随机，而是根据已经学习到的神经网络，尽量选取赢面大的行动。然后，强化学习又反过来使用了改进版本的蒙特卡洛树搜索方法得到的结果来调整自身参数。这样，蒙特卡洛树搜索和强化学习就很好地结合到了一起。

此外，由于去掉了AlphaGo中的第一步有监督学习，AlphaGo Zero完全不需要使用人类专家对局作为数据输入。AlphaGo Zero可以被认为是完全从零开始学习，所需要的只是对其输入围棋基本规则。这一点，对于当前机器学习和人工智能有非常重要的意义。

这里需要解释一下机器学习中有监督学习、强化学习和无监督学习的含义和差别。
广义来讲，所有的问题都可以表示成一个抽象函数，有着输入和输出。机器学习的任务就是从数据中学习出这个函数，至少越来越近似这个函数。

其中，最核心的问题就是数据的样子。理想情况下，我们期望数据是完整的<输入，输出>对，也就是说（近似）正确的输出在数据中被“标注”出来了。标注就是所谓的监督。而在这种情况下的学习，就是有监督学习。而在另一种情况下，数据只包含输入，而不包含输出。也就是完全没有标注，这种情况下的学习就是无监督学习。

但还有一种情况介于这两者之间，其中包括强化学习。强化学习和无监督学习类似，数据中并不包含输出。但是，和无监督学习不同的是，强化学习中的数据也不只包含输入，它还包括了一个数据迭代运行多步之后的奖惩机制。在下棋中，虽然每个棋盘状态下的最佳行动很难给出，但下完之后的输赢很容易判定。这就是奖惩机制，就是一个典型的强化学习问题。强化学习中的奖惩机制一般是在多不迭代后给出，如果只是一步的话，强化学习就变成了有监督学习。所以，从数据的角度看，可以大致认为有监督学习就是一步奖惩的强化学习。

对于某个具体问题，该用哪种学习呢？答案是，都可以用，取决于有什么样的数据。例如下围棋，原则上应该是一个强化学习问题，因为只有最后输赢的奖惩机制才是无异议的。但是，在遇到某个棋盘状态时，会有自己的选择。这些选择虽然不一定是最佳的，但总归大体上都是很好的选择。所以，高手们的棋谱就提供了很多这样的<输入，输出>对。这些，可作为有监督学习的数据。

直观上，数据给的越好越多，那么学习的效果就应该越好。所以有监督学习应该比强化学习奏效，而强化学习应该比无监督学习奏效。这点在实际上也得到了验证。然而，有监督学习需要用到的数据从哪里来？输入好办，但是（近似）正确的输出却很难得到。为了得到正确的输出，往往需要人工来“标注”。机器学习在近十年内取得的巨大成功，离不开相当多在背后默默做标注的人。为了解决“数据”来源的问题，人工智能界主要提出了两条思路。一条就是依靠机器某种方式自动生成和标注数据，如最近很流行的生成对抗网络；另一条就是考虑有监督学习之外的其他机器学习手段，比如强化学习。

AlphaGo Zero证明了后者是完全可行的，直说在机器下围棋的领域。这是否意味着强化学习比有监督学习厉害呢？并不是。除了算法上的改进外，AlphaGo所用到从高手棋谱中得到的数据，即<输入，输出>对（<棋盘状态，选择>对），并不一定是最优解。这是关于数据质量的问题。而强化学习算法，因为不依赖太多先验知识，反而能够跳出圈套。

总而言之，AlphaGo Zero提出了一个新的“基于蒙特卡洛树搜索的强化学习”方法来估算搜索中的评价函数；验证了强化学习的有效性；进一步表明用隐式的方法能够获取显示的知识。

